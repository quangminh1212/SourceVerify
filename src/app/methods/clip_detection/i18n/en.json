{
    "useCase": "Zero-shot and few-shot detection of AI-generated content across multiple generator types",
    "mechanism": "Encodes the input image using CLIP's ViT-L/14 vision encoder to produce a 768-dimensional feature vector. A trained linear probe classifier then determines whether the semantic features match patterns typical of AI-generated content. Leverages CLIP's understanding of visual-semantic relationships.",
    "accuracy": "High - 85-93% across diverse AI generators",
    "name": "CLIP-based AI Detection",
    "source": "Radford et al. (2021) - Learning Transferable Visual Models From Natural Language Supervision, ICML",
    "algorithm": "CLIP ViT-L/14 + Linear Probe Classifier",
    "parameters": "Backbone: ViT-L/14, Feature dim: 768, Classifier: linear probe, Input size: 224x224, Preprocessing: center crop + normalize",
    "description": "Leverages OpenAI CLIP vision-language model to detect semantic inconsistencies between image content and expected real-world properties. Uses contrastive learning for zero-shot detection.",
    "strengths": "• Excellent generalization across AI generators\n• Captures high-level semantic inconsistencies\n• Pre-trained on massive dataset (400M image-text pairs)\n• Works well without fine-tuning",
    "limitations": "• Requires significant computational resources\n• May not catch low-level pixel artifacts\n• Performance varies with image resolution\n• Large model size (~900MB)"
}
